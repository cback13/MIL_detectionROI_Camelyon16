{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812f9ffc-3521-450d-9705-2755f70e891b",
   "metadata": {},
   "source": [
    "# Camelyon_16_extract_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7996b-fd61-481f-86d0-89cf31052d46",
   "metadata": {},
   "source": [
    "##### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa50017a-4de0-470e-bac7-728958b80039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/appa/homes/cbacquie/miniconda3/envs/camelyon/lib/python3.7/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 8.1.2. Several security issues (CVE-2021-27921, CVE-2021-25290, CVE-2021-25291, CVE-2021-25293, and more) have been fixed in pillow 8.1.2 or higher. We recommend to upgrade this library.\n",
      "  from .collection import imread_collection_wrapper\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "import cv2 \n",
    "import openslide\n",
    "from openslide import OpenSlide\n",
    "from openslide import open_slide\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.color import rgb2gray, rgb2hsv\n",
    "from skimage.morphology import area_opening\n",
    "from skimage.exposure import histogram\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.color import rgb2hsv\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8cfee-d83c-47fb-8705-1daf3da8f425",
   "metadata": {},
   "source": [
    "i=1\n",
    "image_path=f\"zeus/Data/Camelyon/tumor/images/{i:03}/tumor_{i:03}.tif\"\n",
    "image_path2=f\"zeus/Data/Camelyon/tumor/images/{i:03}/tumor_{i:03}.png\"\n",
    "img=OpenSlide(image_path)\n",
    "thumbnail=img.get_thumbnail((1000,1000))\n",
    "thumbnail=np.array(thumbnail)\n",
    "thumbnail= thumbnail.astype(np.uint8)\n",
    "Image.fromarray(thumbnail).save(image_path2)\n",
    "a=torch.load(f\"zeus/Data/Camelyon/tumor/features/{i+1:03}/feat_tumor_{i+1:03}.npy\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c5268-5ba1-45c7-91f7-7b3073301b65",
   "metadata": {},
   "source": [
    "#### Create_mask_Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b144b04b-7ed6-4eef-b940-fdc5004a7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask1(thumbnail,path_mask1):\n",
    "    \"\"\"conversion of the image in hsv format, mask on the saturation channel with an otsu filter\n",
    "    thumbnail: thumbnail created with get.thumbnail function\n",
    "    path_mask1:path in which we want to save our mask\n",
    "    return:save the mask as jpg in the given folder and return the mask \"\"\"\n",
    "    hsv = cv2.cvtColor(thumbnail, cv2.COLOR_BGR2HSV)\n",
    "    plt.imshow(hsv[:,:,1])\n",
    "    ret,thresh1 = cv2.threshold(hsv[:,:,1],0,255,cv2.THRESH_OTSU)\n",
    "    images = [hsv, thresh1]\n",
    "    titles=[\"Image\",\"Treshold\"]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(2):\n",
    "        plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n",
    "        plt.title(titles[i])\n",
    "        plt.xticks([]),plt.yticks([])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    kernel=cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
    "    opening = cv2.morphologyEx(thresh1, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    fg_mask1=closing\n",
    "    #Image.fromarray(closing).save(path_mask1)\n",
    "    return fg_mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbfc0cc-14f4-4785-b1cb-617b1c6a9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_mask(thumbnail,fg_mask1,path_final_mask):\n",
    "    \"\"\"superposition of the first mask and our image in order to remove the black elements then otsu filter\n",
    "    thumbnail: thumbnail created with get.thumbnail function,the same as the one used for the first mask\n",
    "    path_final_mask:path in which we want to save our mask\n",
    "    return:save the mask as jpg in the given folder and return the mask \"\"\"\n",
    "    thumbnail2 = thumbnail*np.expand_dims(fg_mask1.astype(np.float32)/255,axis=2).astype(np.uint8)\n",
    "    gray = cv2.cvtColor(thumbnail2, cv2.COLOR_RGB2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0).astype(np.uint8)\n",
    "    # Image.fromarray(blur).save(\"Data/001/patch/blur.jpg\")\n",
    "    ret,thresh1 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    kernel=cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
    "    opening = cv2.morphologyEx(thresh1, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    fg_mask=closing\n",
    "    #Image.fromarray(fg_mask).save(path_final_mask)\n",
    "    fg_mask=closing/255\n",
    "    return fg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15cda6-b327-41ce-9ee4-1d24a5deabe0",
   "metadata": {},
   "source": [
    "#### Extract patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814cde80-6a35-48b5-990d-d13d12e3ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _edge_case(image_shape, y, x, patch_size, step_size):\n",
    "    \"\"\"Keep the tile size at TILE_HEIGHT, TILE_WIDTH \"\"\"\n",
    "    vertical_limit = image_shape[0]-patch_size\n",
    "    horizontal_limit = image_shape[1]-patch_size\n",
    "    new_y = max(0, min(y, vertical_limit))\n",
    "    new_x = max(0, min(x, horizontal_limit))\n",
    "    return new_y,new_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ad815a-6b5e-4904-a7a7-c819aec0eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_patchify_mask(patch_size, overlap, mask, on_mask=0.01):\n",
    "    \"\"\"A simple patches division in grid\"\"\"\n",
    "    patch_dict={}\n",
    "    step_size = int(patch_size-overlap)                                    \n",
    "    index = 0\n",
    "    for y in range(0,mask.shape[0], step_size):                                      \n",
    "        for x in range(0,mask.shape[1], step_size):\n",
    "            # Moves back tile if necessary so they are all of the same size                           \n",
    "            y, x = _edge_case(mask.shape, y, x, patch_size, step_size)\n",
    "            # Fraction of masked pixels\n",
    "            if np.mean(mask[y:y+patch_size, x:x+patch_size]) >= on_mask:\n",
    "                corner = [x, y] \n",
    "                patch = {\"corner\": corner}\n",
    "                patch_dict[index] = patch\n",
    "                index += 1\n",
    "    return patch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4c8c7c-f631-484f-8531-16c2966929f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify_mask(patch_size, overlap, mask, scale=1., on_mask=0.01):\n",
    "    \"\"\"\n",
    "    Adapt patches parameters to the mask scale and patchify.\n",
    "    For best performance, scale, patch_size and overlap should be powers of 2\n",
    "    Scale: original_image_size/mask_size\n",
    "    \"\"\"\n",
    "    scaled_patch_size = int(patch_size//scale)\n",
    "    scaled_overlap = int(overlap//scale)\n",
    "    patch_dict = basic_patchify_mask(scaled_patch_size, scaled_overlap, mask, on_mask=on_mask)\n",
    "    # Rescale\n",
    "    rescaled_patch_dict = {}\n",
    "    for index, patch in patch_dict.items():\n",
    "        corner = patch.get(\"corner\")\n",
    "        rescaled_corner = [int(corner[0]*scale), int(corner[1]*scale)]\n",
    "        rescaled_patch_dict[index] = {\"corner\": rescaled_corner, \"size\": patch_size}\n",
    "    return rescaled_patch_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "953b1632-892c-4327-8f8b-a9ec42986118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixels_extraction_numpy(image, patch):\n",
    "    \"\"\"Get the pixel information for a patch\"\"\"\n",
    "    corner = patch.get(\"corner\")\n",
    "    patch_size = patch.get(\"size\")\n",
    "    # Numpy uses [y, x, c] axis notation, corner is [x, y] \n",
    "    corner.reverse()\n",
    "    pixels = image[\n",
    "            corner[0]:corner[0]+patch_size,\n",
    "            corner[1]:corner[1]+patch_size,\n",
    "            ...\n",
    "            ]\n",
    "    return pixels\n",
    "\n",
    "def pixels_extraction_openslide(image, patch, level=0):\n",
    "    \"\"\"Get the pixel information for every patch\"\"\"\n",
    "    corner = patch.get(\"corner\")\n",
    "    patch_size = patch.get(\"size\")\n",
    "    pixels = image.read_region(location=corner, level=level, size=(patch_size, patch_size))\n",
    "    return pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11493e8e-39da-4fe4-a3c1-1bc0b7929b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patches(image, patch_dict,display_path, line_width=20, scale=1.):\n",
    "    from random import randint\n",
    "    import cv2\n",
    "    display_image = np.asarray(image)\n",
    "    for index, patch in patch_dict.items():\n",
    "        color = [randint(0,255) for _ in range(3)]\n",
    "        scaled_patch_corner = [int(coord//scale) for coord in patch[\"corner\"]]\n",
    "        scaled_patch_size = int(patch[\"size\"]//scale)\n",
    "        bottom_right_corner = [coord+scaled_patch_size for coord in scaled_patch_corner]\n",
    "        # Draw the rescaled patch rectangles\n",
    "        cv2.rectangle(display_image, scaled_patch_corner, bottom_right_corner, color, line_width)\n",
    "        #Image.fromarray(display_image).save(display_path)\n",
    "    return display_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7862212-b731-4a01-8675-7109bcadbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patches(image,patch_dict, directory,basename,level):\n",
    "    \"\"\"Save all images in the patch dictionary\n",
    "    save all images in the directory in .png\"\"\"\n",
    "    L=[]\n",
    "    i=0\n",
    "    for index, patch in tqdm.tqdm(patch_dict.items()):\n",
    "        L.append([i,patch_dict[index]])\n",
    "        i+=1\n",
    "        pixels = pixels_extraction_openslide(image, patch, level=level)\n",
    "        pixels = pixels.convert('RGB')\n",
    "        pixels=pixels.resize((224,224),resample=Image.BILINEAR)\n",
    "        pixels.save((os.path.join(directory, basename+str(index)+'.png')))\n",
    "    return L\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04ae3cd-28e1-4c55-a570-42a6b8fad354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_patch(directory, basename):\n",
    "    \"\"\"show 225 first patches\"\"\"\n",
    "    fig=plt.figure(figsize=(30,30)) \n",
    "    rows = 15\n",
    "    columns = 15\n",
    "    for i in range(1,226):\n",
    "        image = mpimg.imread(os.path.join(directory, basename+str(i)+'.png'))\n",
    "        # print(image.min(),image.max())\n",
    "        a=fig.add_subplot(rows, columns, i) \n",
    "        a.imshow(image) \n",
    "        plt.axis('off') \n",
    "        plt.title(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ba3c4-80d4-4a76-8712-5d35bceef2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok_mask1\n",
      "ok_final_mask\n",
      "ok_patch_dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                          | 649/11199 [00:28<08:51, 19.85it/s]"
     ]
    }
   ],
   "source": [
    "def main(i):\n",
    "    ####Parameters####\n",
    "    patch_size=500\n",
    "    tb_size=1000\n",
    "    overlap=0\n",
    "    level=0\n",
    "    ####images####\n",
    "    image_path=f\"zeus/Data/Camelyon/normal/images/{i:03}/normal_{i:03}.tif\"\n",
    "    img=OpenSlide(image_path)\n",
    "    directory_patch=f\"zeus/Data/Camelyon/normal/images/{i:03}/patch\"\n",
    "    path_mask1=f\"zeus/Data/Camelyon/normal/images/{i:03}/mask/mask1.jpg\"\n",
    "    path_final_mask=f\"zeus/Data/Camelyon/normal/images/{i:03}/mask/final_mask.jpg\"\n",
    "    basename=f\"normal_{i:03}_patch\"\n",
    "    thumbnail=img.get_thumbnail((1000,1000))\n",
    "    thumbnail.show()\n",
    "    display_path=f\"zeus/Data/Camelyon/normal/images/{i:03}/patch/display_patches.jpg\"\n",
    "    thumbnail=np.array(thumbnail)\n",
    "    #print(thumbnail.min(),thumbnail.max())\n",
    "    thumbnail= thumbnail.astype(np.uint8)\n",
    "    ####Parameters####\n",
    "    im_width, im_height = img.dimensions\n",
    "    process_scale = max(im_width,im_height)/1000\n",
    "    scale = img.level_downsamples[level]\n",
    "    ####Get mask1####\n",
    "    fg_mask1=get_mask1(thumbnail,path_mask1)\n",
    "    print(\"ok_mask1\")\n",
    "    ####Get final mask####\n",
    "    fg_mask=get_final_mask(thumbnail,fg_mask1,path_final_mask)\n",
    "    print(\"ok_final_mask\")\n",
    "    ####Get patches####\n",
    "    patch_dict = patchify_mask(int(patch_size*scale),\n",
    "        overlap=overlap, mask=fg_mask, scale=process_scale)\n",
    "    for patch in patch_dict.values():\n",
    "        patch[\"size\"] = int(patch[\"size\"]//scale)\n",
    "    print(\"ok_patch_dict\")\n",
    "    ####Visualize patch on the image####\n",
    "    visualize_patches(thumbnail, patch_dict,display_path, scale=process_scale, line_width=5)\n",
    "    #print(\"ok_display\")\n",
    "    ####Extract pixels and save####\n",
    "    L=save_patches(img, patch_dict,directory_patch,basename,level=level)\n",
    "    np.save(f\"zeus/Data/Camelyon/normal/images/{i:03}/patch_dict.npy\",L)\n",
    "    print(\"ok_save\")\n",
    "    ####Visualize patch####\n",
    "    print(see_patch(directory_patch,basename))\n",
    "if __name__==\"__main__\":\n",
    "    main(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933277a-df3f-4112-b928-6ee949d5cf79",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce3e6b9-cf36-426c-a31e-449896f46b22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:32<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\"tumor\" : transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        \"normal\":  transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        \"test\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "data_dir=\"zeus/Data/Camelyon/\"\n",
    "image_datasets={}\n",
    "dataset_sizes = {}\n",
    "for x in tqdm.tqdm([f\"tumor/images/{i:03}/\" for i in range(1,61)]):\n",
    "    image_datasets[x]=datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[\"tumor\"])\n",
    "    dataset_sizes[x] = len(image_datasets[x])\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3197e64e-91fc-44b9-b009-298a5a2a2049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm.tqdm([f\"normal/images/{i:03}/\" for i in range(1,61)]):\n",
    "    image_datasets[x]=datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[\"normal\"])\n",
    "    dataset_sizes[x] = len(image_datasets[x])\n",
    "#for x in [f\"test/{i:03}/patch\" for i in range(1,21)]:\n",
    " #   image_datasets[x]=datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "  #                                        data_transforms[\"test\"])\n",
    "   # dataset_sizes[x] = len(image_datasets[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73572fb-a1d0-4c00-adfb-a893b07fce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 1053.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 3644.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm.tqdm([f\"tumor/images/{i:03}/\" for i in range(1,61)]):\n",
    "    for i in range(len(image_datasets[x].targets)):\n",
    "        image_datasets[x].targets[i]=0 #tumor=0\n",
    "for x in tqdm.tqdm([f\"normal/images/{i:03}/\" for i in range(1,61)]):\n",
    "    for i in range(len(image_datasets[x].targets)):\n",
    "        image_datasets[x].targets[i]=1 #normal=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1388e-d168-4e4e-bab6-34f266e0a103",
   "metadata": {},
   "source": [
    "#### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeab7a88-27b5-42b7-b2ca-b9b6850df70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 19544.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 33487.46it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloaders ={}\n",
    "for x in tqdm.tqdm([f\"tumor/images/{i:03}/\" for i in range(1,61)]):\n",
    "    dataloaders[x]=torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=False, num_workers=0)\n",
    "for x in tqdm.tqdm([f\"normal/images/{i:03}/\" for i in range(1,61)]):\n",
    "    dataloaders[x]=torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=False, num_workers=0)\n",
    "#for x in [f\"test/{i:03}/patch\" for i in range(1,21)]:\n",
    " #   dataloaders[x]=torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "  #                                           shuffle=False, num_workers=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7fb25-c6af-4dc2-b4ae-5e76d5b19d44",
   "metadata": {},
   "source": [
    "#### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719875cf-9632-4734-a421-e02f0a065fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/appa/homes/cbacquie/miniconda3/envs/camelyon/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/pasteur/appa/homes/cbacquie/miniconda3/envs/camelyon/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Identity(num_ftrs, 2)\n",
    "model_conv = model_conv.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efcfe5e4-9f36-4e5f-b4e5-cb50b5ec33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model,dataloader):\n",
    "    L_features=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs,labels) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            L_features.append(outputs)\n",
    "    return L_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6953a30c-80a8-433b-840d-d5693e6433c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [14:37<00:00, 43.88s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [59:08<00:00, 177.44s/it]\n"
     ]
    }
   ],
   "source": [
    "L_features_normal=[]\n",
    "L_features_tumor=[]\n",
    "for x in tqdm.tqdm([f\"normal/images/{i:03}/\" for i in range(1,61)]):\n",
    "    L_features_normal.append(visualize_model(model_conv,dataloaders[x]))\n",
    "for x in tqdm.tqdm([f\"tumor/images/{i:03}/\" for i in range(1,61)]):\n",
    "    L_features_tumor.append(visualize_model(model_conv,dataloaders[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f724bb52-b6c2-4fb6-a28f-c2347cc9a021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(0,60)):\n",
    "    features_tumor=torch.vstack(L_features_tumor[i])\n",
    "    features_tumor.shape\n",
    "    torch.save(features_tumor,f\"zeus/Data/Camelyon/tumor/features/{i+1:03}/feat_tumor_{i+1:03}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a501386-0bb9-4e51-a284-12abfbfd9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 10.72it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(0,60)):\n",
    "    features_normal=torch.vstack(L_features_normal[i])\n",
    "    features_normal.shape\n",
    "    torch.save(features_normal,f\"zeus/Data/Camelyon/normal/features/{i+1:03}/feat_normal_{i+1:03}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2cae96d-25f0-4561-84f2-6f3ff7fd3e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 19.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1852, 2048]), torch.Size([1188, 2048]), torch.Size([4743, 2048]), torch.Size([310, 2048]), torch.Size([1335, 2048]), torch.Size([1226, 2048]), torch.Size([3118, 2048]), torch.Size([597, 2048]), torch.Size([7349, 2048]), torch.Size([1987, 2048]), torch.Size([11148, 2048]), torch.Size([3339, 2048]), torch.Size([609, 2048]), torch.Size([2069, 2048]), torch.Size([4802, 2048]), torch.Size([252, 2048]), torch.Size([407, 2048]), torch.Size([2965, 2048]), torch.Size([241, 2048]), torch.Size([558, 2048])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#verification qu'on a bien des tensors de taille nx2048 \n",
    "L=[]\n",
    "for i in tqdm.tqdm(range(0,60)):\n",
    "    a=torch.load(f\"zeus/Data/Camelyon/normal/features/{i+1:03}/feat_normal_{i+1:03}.npy\")\n",
    "    L.append(a.shape)\n",
    "print(L)\n",
    "#ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375e04f-b3f4-4d76-bfc4-7ce19cb2e052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([6358, 2048]), torch.Size([3952, 2048]), torch.Size([5795, 2048]), torch.Size([11532, 2048]), torch.Size([3570, 2048]), torch.Size([15334, 2048]), torch.Size([11017, 2048]), torch.Size([11729, 2048]), torch.Size([10463, 2048]), torch.Size([8533, 2048]), torch.Size([4609, 2048]), torch.Size([6346, 2048]), torch.Size([5078, 2048]), torch.Size([9677, 2048]), torch.Size([27134, 2048]), torch.Size([4631, 2048]), torch.Size([8407, 2048]), torch.Size([8740, 2048]), torch.Size([2275, 2048]), torch.Size([6218, 2048])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#verification qu'on a bien des tensors de taille nx2048 \n",
    "L=[]\n",
    "for i in tqdm.tqdm(range(0,60)):\n",
    "    a=torch.load(f\"zeus/Data/Camelyon/tumor/features/{i+1:03}/feat_tumor_{i+1:03}.npy\")\n",
    "    L.append(a.shape) \n",
    "print(L)\n",
    "#ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a1d07-d265-410f-a2bb-95036f534558",
   "metadata": {},
   "source": [
    "#### Create Dataset Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f349028b-8613-4e0f-88c0-e47a30758c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pandas dataset filename+y\n",
    "datadict={}\n",
    "Lfilename=[]\n",
    "Ly=[]\n",
    "for x in [f\"zeus/Data/Camelyon/normal/features/{i+1:03}/feat_normal_{i+1:03}.npy\" for i in range(0,60)]:\n",
    "    Lfilename.append(x)\n",
    "    Ly.append(1) #normal=1\n",
    "    \n",
    "for x in [f\"zeus/Data/Camelyon/tumor/features/{i+1:03}/feat_tumor_{i+1:03}.npy\" for i in range(0,60)]:\n",
    "    Lfilename.append(x)\n",
    "    Ly.append(0) #tumor=0\n",
    "datadict['filename']=Lfilename\n",
    "datadict['y']=Ly\n",
    "dataset= pd.DataFrame(data=datadict)\n",
    "#dataset[\"filename\"][3]\n",
    "#dataset[dataset[\"filename\"]==dataset[\"filename\"][3]][\"y\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d65a6b-cd1c-46bc-8080-4fa926074c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomImageDataset(Dataset):  #générateur\n",
    "    def __init__(self, df_filename):\n",
    "        self.df_filename = df_filename\n",
    "    def __len__(self):\n",
    "        return len(self.df_filename)\n",
    "    def __getitem__(self, idx):\n",
    "        filename_label = self.df_filename.loc[idx]\n",
    "        filename=filename_label[\"filename\"]\n",
    "        label=filename_label[\"y\"]\n",
    "        matrix = torch.load(filename,map_location=\"cpu\") #load features\n",
    "        #print(matrix.shape,flush=True)\n",
    "        #indices=np.random.choice(np.arange(matrix.shape[0]),5000, replace=False)#prendre 5000 patchs dans la matrice  \n",
    "        #matrix=matrix[indices]\n",
    "        return matrix, label\n",
    "dataset_features={'train':CustomImageDataset(dataset)}\n",
    "dataset_sizes=len(dataset_features['train'])\n",
    "#CustomImageDataset(dataset)[4][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b3ee10-ed0d-4214-9e51-aa1fb619ec1b",
   "metadata": {},
   "source": [
    "#### Create Dataloaders Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76e72a2-36b3-4421-8889-5c3901af5ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8740, 2048])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataloaders\n",
    "def collate_fn(batch):    \n",
    "    data = [item[0] for item in batch]\n",
    "    data = pack_sequence(data, enforce_sorted=False)[0]\n",
    "    targets = torch.Tensor([item[1] for item in batch])\n",
    "    return [data, targets]\n",
    "\n",
    "dataloaders_features={'train':torch.utils.data.DataLoader(dataset_features['train'], \n",
    "                                batch_size=1, num_workers=1, shuffle=True)}\n",
    "next(iter(dataloaders_features['train']))[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807739b-4722-46b4-b282-3cacc9636b76",
   "metadata": {},
   "source": [
    "#### Convolutionnal 1-1 Layer+min,max Layer+MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438008f4-e0de-4da8-a0dd-242c9f7a3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b051865f-67ab-44f8-a388-310ad5bb60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv2d=nn.Conv2d(in_channels=2048, out_channels=1, kernel_size=1)\n",
    "        mlp1 = nn.Linear(10, 200)\n",
    "        mlp2 = nn.Linear(200, 100)\n",
    "        mlp3 = nn.Linear(100, 1)\n",
    "        self.mlp = nn.Sequential(mlp1,nn.Sigmoid(),nn.Dropout(0.2),\n",
    "                                 mlp2,nn.Sigmoid(),nn.Dropout(0.2),\n",
    "                                 mlp3)\n",
    "    def forward(self, x):\n",
    "        x=self.conv2d(x)\n",
    "        #print(x.shape)\n",
    "        x=x.reshape(1,-1)\n",
    "        #\n",
    "        min_x,indicemin=(torch.topk(x, 5, dim=1, largest=False,sorted=True))\n",
    "        max_x,indicemax=(torch.topk(x, 5, dim=1, largest=True,sorted=True))\n",
    "        output=torch.hstack((min_x,max_x))\n",
    "        #print(output.shape)\n",
    "        y_pred = self.mlp(output)\n",
    "        return y_pred\n",
    "                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a28f6-6f44-4a13-b654-1b2ed3fad3eb",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1042e097-13c9-4216-8588-5b3e8dd0ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.6988 Acc: 0.4750\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.7138 Acc: 0.4750\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.7181 Acc: 0.4250\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.6987 Acc: 0.5750\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.6963 Acc: 0.5500\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.7210 Acc: 0.4500\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.6894 Acc: 0.5250\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.7235 Acc: 0.3750\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.6956 Acc: 0.6250\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.6987 Acc: 0.5250\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.7034 Acc: 0.4750\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.6807 Acc: 0.6750\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.6961 Acc: 0.4750\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.7023 Acc: 0.4750\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.6975 Acc: 0.4250\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.7037 Acc: 0.5500\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.6968 Acc: 0.5500\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.6962 Acc: 0.5250\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.7204 Acc: 0.3500\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.6912 Acc: 0.4500\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.6910 Acc: 0.4750\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.6762 Acc: 0.5500\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.6939 Acc: 0.5250\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.6930 Acc: 0.4750\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.6831 Acc: 0.6500\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.6847 Acc: 0.5250\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.7106 Acc: 0.4000\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.6876 Acc: 0.5750\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.6897 Acc: 0.5500\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.7046 Acc: 0.4500\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.6927 Acc: 0.5250\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.6835 Acc: 0.5250\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.6999 Acc: 0.5000\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.6685 Acc: 0.6250\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.7028 Acc: 0.4500\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.7015 Acc: 0.6250\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.6952 Acc: 0.5250\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.6821 Acc: 0.5000\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.7015 Acc: 0.5500\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.6900 Acc: 0.5750\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.7031 Acc: 0.5250\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.6781 Acc: 0.5750\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.6832 Acc: 0.6000\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.7025 Acc: 0.4500\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.6853 Acc: 0.6000\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.7045 Acc: 0.4250\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.6904 Acc: 0.5250\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.6817 Acc: 0.5250\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.7194 Acc: 0.4250\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.7040 Acc: 0.4250\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.6946 Acc: 0.5250\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.4750\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.6883 Acc: 0.6000\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.6888 Acc: 0.5250\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.6830 Acc: 0.5250\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.6892 Acc: 0.4500\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.6884 Acc: 0.5750\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.7058 Acc: 0.4250\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.6980 Acc: 0.5250\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.7062 Acc: 0.4250\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.7033 Acc: 0.4000\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.6866 Acc: 0.5500\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.7063 Acc: 0.5500\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.6867 Acc: 0.5500\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.6961 Acc: 0.4750\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.6866 Acc: 0.6000\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.6888 Acc: 0.4750\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.6806 Acc: 0.6250\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.5250\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.6886 Acc: 0.5500\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.6999 Acc: 0.4750\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.6975 Acc: 0.4250\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.6858 Acc: 0.6000\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.6819 Acc: 0.5500\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.7255 Acc: 0.3250\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.6925 Acc: 0.5000\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.7030 Acc: 0.4500\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.7087 Acc: 0.4000\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.7157 Acc: 0.4250\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.6928 Acc: 0.5000\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.6811 Acc: 0.5500\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.7052 Acc: 0.5250\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.6724 Acc: 0.6500\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.6882 Acc: 0.5500\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.6880 Acc: 0.5250\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.7093 Acc: 0.4000\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.7147 Acc: 0.4000\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.6820 Acc: 0.5250\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.6880 Acc: 0.5250\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.6893 Acc: 0.5500\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.6662 Acc: 0.6000\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.6820 Acc: 0.6000\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.6828 Acc: 0.5000\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.6752 Acc: 0.6750\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.6993 Acc: 0.4750\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.6994 Acc: 0.4500\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.7051 Acc: 0.4500\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.7100 Acc: 0.4500\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.6996 Acc: 0.6000\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.6918 Acc: 0.6500\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.7041 Acc: 0.4250\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.7097 Acc: 0.3250\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.7026 Acc: 0.4000\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.6892 Acc: 0.5500\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.6958 Acc: 0.4750\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.6628 Acc: 0.6500\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.7015 Acc: 0.5000\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.6806 Acc: 0.5500\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.6746 Acc: 0.7000\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.7087 Acc: 0.4750\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.6907 Acc: 0.4750\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.6918 Acc: 0.4500\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.7193 Acc: 0.4250\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.7020 Acc: 0.4750\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.6849 Acc: 0.5250\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.7096 Acc: 0.5000\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.6887 Acc: 0.5750\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.6948 Acc: 0.4500\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.6977 Acc: 0.4750\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.6963 Acc: 0.4500\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.7014 Acc: 0.5500\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.6926 Acc: 0.5250\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.6947 Acc: 0.5250\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.6710 Acc: 0.6500\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.6777 Acc: 0.6250\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.6854 Acc: 0.5500\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.6945 Acc: 0.5500\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.6833 Acc: 0.5500\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.7158 Acc: 0.5000\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.7126 Acc: 0.4500\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.6873 Acc: 0.6250\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.6885 Acc: 0.5000\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.7084 Acc: 0.4500\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.6898 Acc: 0.5750\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.6926 Acc: 0.5000\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.7033 Acc: 0.4750\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.7094 Acc: 0.4500\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.6812 Acc: 0.6000\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.7094 Acc: 0.3250\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.6829 Acc: 0.4750\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.6698 Acc: 0.5500\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.6951 Acc: 0.5500\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.6905 Acc: 0.6250\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.6842 Acc: 0.5500\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.6951 Acc: 0.5000\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.6824 Acc: 0.6000\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.6885 Acc: 0.5750\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.6925 Acc: 0.6000\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.6852 Acc: 0.5250\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.7014 Acc: 0.4500\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.6946 Acc: 0.5750\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.6792 Acc: 0.5250\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.6631 Acc: 0.7500\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.6775 Acc: 0.6750\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.7087 Acc: 0.4500\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.6943 Acc: 0.5500\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.6981 Acc: 0.5250\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.6958 Acc: 0.5000\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.6896 Acc: 0.5250\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.7021 Acc: 0.4750\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.6938 Acc: 0.5250\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.6926 Acc: 0.6000\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.6897 Acc: 0.5250\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.7009 Acc: 0.4000\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.7024 Acc: 0.5000\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.7145 Acc: 0.3750\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.7041 Acc: 0.4500\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.6819 Acc: 0.5750\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.7130 Acc: 0.4750\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.6989 Acc: 0.4750\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.6999 Acc: 0.5750\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.7038 Acc: 0.5500\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.7170 Acc: 0.4000\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.6752 Acc: 0.6750\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.6970 Acc: 0.5000\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.6872 Acc: 0.4750\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.6952 Acc: 0.5250\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.6943 Acc: 0.5500\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.6939 Acc: 0.4000\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.6823 Acc: 0.6000\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.6967 Acc: 0.4750\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.7078 Acc: 0.4500\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.6937 Acc: 0.4500\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.6936 Acc: 0.4750\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.6864 Acc: 0.6500\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.6000\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.6872 Acc: 0.5500\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.6876 Acc: 0.6000\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.6890 Acc: 0.4250\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.7026 Acc: 0.5250\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.6894 Acc: 0.5750\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.7056 Acc: 0.3750\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.7005 Acc: 0.4750\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.6785 Acc: 0.4750\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.6856 Acc: 0.5500\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.6799 Acc: 0.6250\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.7019 Acc: 0.4750\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.7150 Acc: 0.4000\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.6964 Acc: 0.5250\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.6586 Acc: 0.6750\n",
      "\n",
      "Training complete in 17m 29s\n",
      "Best val Acc: 0.000000\n",
      "Network(\n",
      "  (conv2d): Conv2d(2048, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=200, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (4): Sigmoid()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Network().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    inputs=inputs.reshape(1,2048,-1, 1)\n",
    "                    #print(inputs.shape)\n",
    "                    outputs = model(inputs)\n",
    "                    preds=torch.sigmoid(outputs)>0.5\n",
    "                    loss = criterion(outputs, labels.unsqueeze(0).float())\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'train' and epoch_acc > best_acc: #remettre valid apres\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "print(train_model(model, criterion, optimizer, exp_lr_scheduler, dataloaders_features, num_epochs=200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943adcac-d64a-471a-9b23-50744042f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\"\n",
    "model = Network().to(device)\n",
    "for inputs, labels in dataloaders_features['train']:\n",
    "                inputs = inputs.to(device)\n",
    "                inputs=inputs.reshape(1,2048,-1, 1)\n",
    "                labels = labels.to(device)\n",
    "                a=model.conv2d(inputs)\n",
    "                torch.histc(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2bdee-fc5d-4687-8296-d0532da9e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_labels = []\n",
    "local_scores = []\n",
    "coords = []\n",
    "\n",
    "for img, g, labels, xy in validation_dataloader:\n",
    "    scores = model.embedding(img).view(-1).detach().cpu().numpy()\n",
    "    local_scores.append(scores)\n",
    "    local_labels.append(labels.view(-1).numpy())\n",
    "    coords.append(xy)\n",
    "    \n",
    "    \n",
    "    if g == 1:\n",
    "#         plt.figure(figsize=(20,10))\n",
    "#         plt.hist(scores, bins=100, density=True)\n",
    "#         plt.yscale(\"log\")\n",
    "#         plt.show()\n",
    "        plotMap(labels.view(-1).numpy(), scores<-0., xy[0].cpu().numpy())\n",
    "\n",
    "# local_labels, local_scores = np.array(local_labels), np.array(local_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad23a9-0962-4ea7-b10a-81a8a44cf302",
   "metadata": {},
   "outputs": [],
   "source": [
    "echnatillonnage uniforme ne pas prendre les images ou il y a peu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
