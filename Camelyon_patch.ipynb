{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812f9ffc-3521-450d-9705-2755f70e891b",
   "metadata": {},
   "source": [
    "# Camelyon_16_extract_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7996b-fd61-481f-86d0-89cf31052d46",
   "metadata": {},
   "source": [
    "##### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa50017a-4de0-470e-bac7-728958b80039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/appa/homes/cbacquie/miniconda3/envs/camelyon/lib/python3.7/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 8.1.2. Several security issues (CVE-2021-27921, CVE-2021-25290, CVE-2021-25291, CVE-2021-25293, and more) have been fixed in pillow 8.1.2 or higher. We recommend to upgrade this library.\n",
      "  from .collection import imread_collection_wrapper\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "import cv2 \n",
    "import openslide\n",
    "from openslide import OpenSlide\n",
    "from openslide import open_slide\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.color import rgb2gray, rgb2hsv\n",
    "from skimage.morphology import area_opening\n",
    "from skimage.exposure import histogram\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.color import rgb2hsv\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8cfee-d83c-47fb-8705-1daf3da8f425",
   "metadata": {},
   "source": [
    "i=1\n",
    "image_path=f\"zeus/Data/Camelyon/tumor/images/{i:03}/tumor_{i:03}.tif\"\n",
    "image_path2=f\"zeus/Data/Camelyon/tumor/images/{i:03}/tumor_{i:03}.png\"\n",
    "img=OpenSlide(image_path)\n",
    "thumbnail=img.get_thumbnail((1000,1000))\n",
    "thumbnail=np.array(thumbnail)\n",
    "thumbnail= thumbnail.astype(np.uint8)\n",
    "Image.fromarray(thumbnail).save(image_path2)\n",
    "a=torch.load(f\"zeus/Data/Camelyon/tumor/features/{i+1:03}/feat_tumor_{i+1:03}.npy\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c5268-5ba1-45c7-91f7-7b3073301b65",
   "metadata": {},
   "source": [
    "#### Create_mask_Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b144b04b-7ed6-4eef-b940-fdc5004a7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask1(thumbnail,path_mask1):\n",
    "    \"\"\"conversion of the image in hsv format, mask on the saturation channel with an otsu filter\n",
    "    thumbnail: thumbnail created with get.thumbnail function\n",
    "    path_mask1:path in which we want to save our mask\n",
    "    return:save the mask as jpg in the given folder and return the mask \"\"\"\n",
    "    hsv = cv2.cvtColor(thumbnail, cv2.COLOR_BGR2HSV)\n",
    "    plt.imshow(hsv[:,:,1])\n",
    "    ret,thresh1 = cv2.threshold(hsv[:,:,1],0,255,cv2.THRESH_OTSU)\n",
    "    images = [hsv, thresh1]\n",
    "    titles=[\"Image\",\"Treshold\"]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(2):\n",
    "        plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n",
    "        plt.title(titles[i])\n",
    "        plt.xticks([]),plt.yticks([])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    kernel=cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
    "    opening = cv2.morphologyEx(thresh1, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    fg_mask1=closing\n",
    "    #Image.fromarray(closing).save(path_mask1)\n",
    "    return fg_mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbfc0cc-14f4-4785-b1cb-617b1c6a9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_mask(thumbnail,fg_mask1,path_final_mask):\n",
    "    \"\"\"superposition of the first mask and our image in order to remove the black elements then otsu filter\n",
    "    thumbnail: thumbnail created with get.thumbnail function,the same as the one used for the first mask\n",
    "    path_final_mask:path in which we want to save our mask\n",
    "    return:save the mask as jpg in the given folder and return the mask \"\"\"\n",
    "    thumbnail2 = thumbnail*np.expand_dims(fg_mask1.astype(np.float32)/255,axis=2).astype(np.uint8)\n",
    "    gray = cv2.cvtColor(thumbnail2, cv2.COLOR_RGB2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5,5), 0).astype(np.uint8)\n",
    "    # Image.fromarray(blur).save(\"Data/001/patch/blur.jpg\")\n",
    "    ret,thresh1 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    kernel=cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
    "    opening = cv2.morphologyEx(thresh1, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "    fg_mask=closing\n",
    "    #Image.fromarray(fg_mask).save(path_final_mask)\n",
    "    fg_mask=closing/255\n",
    "    return fg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15cda6-b327-41ce-9ee4-1d24a5deabe0",
   "metadata": {},
   "source": [
    "#### Extract patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814cde80-6a35-48b5-990d-d13d12e3ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _edge_case(image_shape, y, x, patch_size, step_size):\n",
    "    \"\"\"Keep the tile size at TILE_HEIGHT, TILE_WIDTH \"\"\"\n",
    "    vertical_limit = image_shape[0]-patch_size\n",
    "    horizontal_limit = image_shape[1]-patch_size\n",
    "    new_y = max(0, min(y, vertical_limit))\n",
    "    new_x = max(0, min(x, horizontal_limit))\n",
    "    return new_y,new_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ad815a-6b5e-4904-a7a7-c819aec0eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_patchify_mask(patch_size, overlap, mask, on_mask=0.01):\n",
    "    \"\"\"A simple patches division in grid\"\"\"\n",
    "    patch_dict={}\n",
    "    step_size = int(patch_size-overlap)                                    \n",
    "    index = 0\n",
    "    for y in range(0,mask.shape[0], step_size):                                      \n",
    "        for x in range(0,mask.shape[1], step_size):\n",
    "            # Moves back tile if necessary so they are all of the same size                           \n",
    "            y, x = _edge_case(mask.shape, y, x, patch_size, step_size)\n",
    "            # Fraction of masked pixels\n",
    "            if np.mean(mask[y:y+patch_size, x:x+patch_size]) >= on_mask:\n",
    "                corner = [x, y] \n",
    "                patch = {\"corner\": corner}\n",
    "                patch_dict[index] = patch\n",
    "                index += 1\n",
    "    return patch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4c8c7c-f631-484f-8531-16c2966929f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify_mask(patch_size, overlap, mask, scale=1., on_mask=0.01):\n",
    "    \"\"\"\n",
    "    Adapt patches parameters to the mask scale and patchify.\n",
    "    For best performance, scale, patch_size and overlap should be powers of 2\n",
    "    Scale: original_image_size/mask_size\n",
    "    \"\"\"\n",
    "    scaled_patch_size = int(patch_size//scale)\n",
    "    scaled_overlap = int(overlap//scale)\n",
    "    patch_dict = basic_patchify_mask(scaled_patch_size, scaled_overlap, mask, on_mask=on_mask)\n",
    "    # Rescale\n",
    "    rescaled_patch_dict = {}\n",
    "    for index, patch in patch_dict.items():\n",
    "        corner = patch.get(\"corner\")\n",
    "        rescaled_corner = [int(corner[0]*scale), int(corner[1]*scale)]\n",
    "        rescaled_patch_dict[index] = {\"corner\": rescaled_corner, \"size\": patch_size}\n",
    "    return rescaled_patch_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "953b1632-892c-4327-8f8b-a9ec42986118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixels_extraction_numpy(image, patch):\n",
    "    \"\"\"Get the pixel information for a patch\"\"\"\n",
    "    corner = patch.get(\"corner\")\n",
    "    patch_size = patch.get(\"size\")\n",
    "    # Numpy uses [y, x, c] axis notation, corner is [x, y] \n",
    "    corner.reverse()\n",
    "    pixels = image[\n",
    "            corner[0]:corner[0]+patch_size,\n",
    "            corner[1]:corner[1]+patch_size,\n",
    "            ...\n",
    "            ]\n",
    "    return pixels\n",
    "\n",
    "def pixels_extraction_openslide(image, patch, level=0):\n",
    "    \"\"\"Get the pixel information for every patch\"\"\"\n",
    "    corner = patch.get(\"corner\")\n",
    "    patch_size = patch.get(\"size\")\n",
    "    pixels = image.read_region(location=corner, level=level, size=(patch_size, patch_size))\n",
    "    return pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11493e8e-39da-4fe4-a3c1-1bc0b7929b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patches(image, patch_dict,display_path, line_width=20, scale=1.):\n",
    "    from random import randint\n",
    "    import cv2\n",
    "    display_image = np.asarray(image)\n",
    "    for index, patch in patch_dict.items():\n",
    "        color = [randint(0,255) for _ in range(3)]\n",
    "        scaled_patch_corner = [int(coord//scale) for coord in patch[\"corner\"]]\n",
    "        scaled_patch_size = int(patch[\"size\"]//scale)\n",
    "        bottom_right_corner = [coord+scaled_patch_size for coord in scaled_patch_corner]\n",
    "        # Draw the rescaled patch rectangles\n",
    "        cv2.rectangle(display_image, scaled_patch_corner, bottom_right_corner, color, line_width)\n",
    "        #Image.fromarray(display_image).save(display_path)\n",
    "    return display_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7862212-b731-4a01-8675-7109bcadbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patches(image,patch_dict, directory,basename,level):\n",
    "    \"\"\"Save all images in the patch dictionary\n",
    "    save all images in the directory in .png\"\"\"\n",
    "    L=[]\n",
    "    i=0\n",
    "    for index, patch in tqdm.tqdm(patch_dict.items()):\n",
    "        L.append([i,patch_dict[index]])\n",
    "        i+=1\n",
    "        pixels = pixels_extraction_openslide(image, patch, level=level)\n",
    "        pixels = pixels.convert('RGB')\n",
    "        pixels=pixels.resize((224,224),resample=Image.BILINEAR)\n",
    "        pixels.save((os.path.join(directory, basename+str(index)+'.png')))\n",
    "    return L\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04ae3cd-28e1-4c55-a570-42a6b8fad354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_patch(directory, basename):\n",
    "    \"\"\"show 225 first patches\"\"\"\n",
    "    fig=plt.figure(figsize=(30,30)) \n",
    "    rows = 15\n",
    "    columns = 15\n",
    "    for i in range(1,226):\n",
    "        image = mpimg.imread(os.path.join(directory, basename+str(i)+'.png'))\n",
    "        # print(image.min(),image.max())\n",
    "        a=fig.add_subplot(rows, columns, i) \n",
    "        a.imshow(image) \n",
    "        plt.axis('off') \n",
    "        plt.title(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ba3c4-80d4-4a76-8712-5d35bceef2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok_mask1\n",
      "ok_final_mask\n",
      "ok_patch_dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                          | 649/11199 [00:28<08:51, 19.85it/s]"
     ]
    }
   ],
   "source": [
    "def main(i):\n",
    "    ####Parameters####\n",
    "    patch_size=500\n",
    "    tb_size=1000\n",
    "    overlap=0\n",
    "    level=0\n",
    "    ####images####\n",
    "    image_path=f\"zeus/Data/Camelyon/normal/images/{i:03}/normal_{i:03}.tif\"\n",
    "    img=OpenSlide(image_path)\n",
    "    directory_patch=f\"zeus/Data/Camelyon/normal/images/{i:03}/patch\"\n",
    "    path_mask1=f\"zeus/Data/Camelyon/normal/images/{i:03}/mask/mask1.jpg\"\n",
    "    path_final_mask=f\"zeus/Data/Camelyon/normal/images/{i:03}/mask/final_mask.jpg\"\n",
    "    basename=f\"normal_{i:03}_patch\"\n",
    "    thumbnail=img.get_thumbnail((1000,1000))\n",
    "    thumbnail.show()\n",
    "    display_path=f\"zeus/Data/Camelyon/normal/images/{i:03}/patch/display_patches.jpg\"\n",
    "    thumbnail=np.array(thumbnail)\n",
    "    #print(thumbnail.min(),thumbnail.max())\n",
    "    thumbnail= thumbnail.astype(np.uint8)\n",
    "    ####Parameters####\n",
    "    im_width, im_height = img.dimensions\n",
    "    process_scale = max(im_width,im_height)/1000\n",
    "    scale = img.level_downsamples[level]\n",
    "    ####Get mask1####\n",
    "    fg_mask1=get_mask1(thumbnail,path_mask1)\n",
    "    print(\"ok_mask1\")\n",
    "    ####Get final mask####\n",
    "    fg_mask=get_final_mask(thumbnail,fg_mask1,path_final_mask)\n",
    "    print(\"ok_final_mask\")\n",
    "    ####Get patches####\n",
    "    patch_dict = patchify_mask(int(patch_size*scale),\n",
    "        overlap=overlap, mask=fg_mask, scale=process_scale)\n",
    "    for patch in patch_dict.values():\n",
    "        patch[\"size\"] = int(patch[\"size\"]//scale)\n",
    "    print(\"ok_patch_dict\")\n",
    "    ####Visualize patch on the image####\n",
    "    visualize_patches(thumbnail, patch_dict,display_path, scale=process_scale, line_width=5)\n",
    "    #print(\"ok_display\")\n",
    "    ####Extract pixels and save####\n",
    "    L=save_patches(img, patch_dict,directory_patch,basename,level=level)\n",
    "    np.save(f\"zeus/Data/Camelyon/normal/images/{i:03}/patch_dict.npy\",L)\n",
    "    print(\"ok_save\")\n",
    "    ####Visualize patch####\n",
    "    print(see_patch(directory_patch,basename))\n",
    "if __name__==\"__main__\":\n",
    "    main(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933277a-df3f-4112-b928-6ee949d5cf79",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce3e6b9-cf36-426c-a31e-449896f46b22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:32<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\"tumor\" : transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        \"normal\":  transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        \"test\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "data_dir=\"zeus/Data/Camelyon/\"\n",
    "image_datasets={}\n",
    "dataset_sizes = {}\n",
    "for x in tqdm.tqdm([f\"tumor/images/{i:03}/\" for i in range(1,61)]):\n",
    "    image_datasets[x]=datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[\"tumor\"])\n",
    "    dataset_sizes[x] = len(image_datasets[x])\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3197e64e-91fc-44b9-b009-298a5a2a2049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm.tqdm([f\"normal/images/{i:03}/\" for i in range(1,61)]):\n",
    "    image_datasets[x]=datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[\"normal\"])\n",
    "    dataset_sizes[x] = len(image_datasets[x])\n",
    "#for x in [f\"test/{i:03}/patch\" for i in range(1,21)]:\n",
    " #   image_datasets[x]=datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "  #                                        data_transforms[\"test\"])\n",
    "   # dataset_sizes[x] = len(image_datasets[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73572fb-a1d0-4c00-adfb-a893b07fce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 1053.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 3644.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm.tqdm([f\"tumor/images/{i:03}/\" for i in range(1,61)]):\n",
    "    for i in range(len(image_datasets[x].targets)):\n",
    "        image_datasets[x].targets[i]=0 #tumor=0\n",
    "for x in tqdm.tqdm([f\"normal/images/{i:03}/\" for i in range(1,61)]):\n",
    "    for i in range(len(image_datasets[x].targets)):\n",
    "        image_datasets[x].targets[i]=1 #normal=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1388e-d168-4e4e-bab6-34f266e0a103",
   "metadata": {},
   "source": [
    "#### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeab7a88-27b5-42b7-b2ca-b9b6850df70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 19544.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 33487.46it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloaders ={}\n",
    "for x in tqdm.tqdm([f\"tumor/images/{i:03}/\" for i in range(1,61)]):\n",
    "    dataloaders[x]=torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=False, num_workers=0)\n",
    "for x in tqdm.tqdm([f\"normal/images/{i:03}/\" for i in range(1,61)]):\n",
    "    dataloaders[x]=torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=False, num_workers=0)\n",
    "#for x in [f\"test/{i:03}/patch\" for i in range(1,21)]:\n",
    " #   dataloaders[x]=torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "  #                                           shuffle=False, num_workers=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7fb25-c6af-4dc2-b4ae-5e76d5b19d44",
   "metadata": {},
   "source": [
    "#### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719875cf-9632-4734-a421-e02f0a065fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/appa/homes/cbacquie/miniconda3/envs/camelyon/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/pasteur/appa/homes/cbacquie/miniconda3/envs/camelyon/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Identity(num_ftrs, 2)\n",
    "model_conv = model_conv.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efcfe5e4-9f36-4e5f-b4e5-cb50b5ec33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model,dataloader):\n",
    "    L_features=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs,labels) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            L_features.append(outputs)\n",
    "    return L_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6953a30c-80a8-433b-840d-d5693e6433c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [14:37<00:00, 43.88s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [59:08<00:00, 177.44s/it]\n"
     ]
    }
   ],
   "source": [
    "L_features_normal=[]\n",
    "L_features_tumor=[]\n",
    "for x in tqdm.tqdm([f\"normal/images/{i:03}/\" for i in range(1,61)]):\n",
    "    L_features_normal.append(visualize_model(model_conv,dataloaders[x]))\n",
    "for x in tqdm.tqdm([f\"tumor/images/{i:03}/\" for i in range(1,61)]):\n",
    "    L_features_tumor.append(visualize_model(model_conv,dataloaders[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f724bb52-b6c2-4fb6-a28f-c2347cc9a021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(0,60)):\n",
    "    features_tumor=torch.vstack(L_features_tumor[i])\n",
    "    features_tumor.shape\n",
    "    torch.save(features_tumor,f\"zeus/Data/Camelyon/tumor/features/{i+1:03}/feat_tumor_{i+1:03}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a501386-0bb9-4e51-a284-12abfbfd9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 10.72it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(0,60)):\n",
    "    features_normal=torch.vstack(L_features_normal[i])\n",
    "    features_normal.shape\n",
    "    torch.save(features_normal,f\"zeus/Data/Camelyon/normal/features/{i+1:03}/feat_normal_{i+1:03}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2cae96d-25f0-4561-84f2-6f3ff7fd3e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 19.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1852, 2048]), torch.Size([1188, 2048]), torch.Size([4743, 2048]), torch.Size([310, 2048]), torch.Size([1335, 2048]), torch.Size([1226, 2048]), torch.Size([3118, 2048]), torch.Size([597, 2048]), torch.Size([7349, 2048]), torch.Size([1987, 2048]), torch.Size([11148, 2048]), torch.Size([3339, 2048]), torch.Size([609, 2048]), torch.Size([2069, 2048]), torch.Size([4802, 2048]), torch.Size([252, 2048]), torch.Size([407, 2048]), torch.Size([2965, 2048]), torch.Size([241, 2048]), torch.Size([558, 2048])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#verification qu'on a bien des tensors de taille nx2048 \n",
    "L=[]\n",
    "for i in tqdm.tqdm(range(0,60)):\n",
    "    a=torch.load(f\"zeus/Data/Camelyon/normal/features/{i+1:03}/feat_normal_{i+1:03}.npy\")\n",
    "    L.append(a.shape)\n",
    "print(L)\n",
    "#ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375e04f-b3f4-4d76-bfc4-7ce19cb2e052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([6358, 2048]), torch.Size([3952, 2048]), torch.Size([5795, 2048]), torch.Size([11532, 2048]), torch.Size([3570, 2048]), torch.Size([15334, 2048]), torch.Size([11017, 2048]), torch.Size([11729, 2048]), torch.Size([10463, 2048]), torch.Size([8533, 2048]), torch.Size([4609, 2048]), torch.Size([6346, 2048]), torch.Size([5078, 2048]), torch.Size([9677, 2048]), torch.Size([27134, 2048]), torch.Size([4631, 2048]), torch.Size([8407, 2048]), torch.Size([8740, 2048]), torch.Size([2275, 2048]), torch.Size([6218, 2048])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#verification qu'on a bien des tensors de taille nx2048 \n",
    "L=[]\n",
    "for i in tqdm.tqdm(range(0,60)):\n",
    "    a=torch.load(f\"zeus/Data/Camelyon/tumor/features/{i+1:03}/feat_tumor_{i+1:03}.npy\")\n",
    "    L.append(a.shape) \n",
    "print(L)\n",
    "#ok"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
